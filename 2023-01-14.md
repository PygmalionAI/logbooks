# 2023-01-14

Tales of trying out SFT.

## Experiment 4

### What's new

A little more CC data compared to the last run, plus I'm using convogpt's SFT loss implementation instead of a regular CLM loss as the training objective this time.

In more straightforward terms: the model was trained to predict responses to a given input, rather than trying to learn to predict the response _and_ the input prompt all mushed together.

### Training

As an initial note, the SFT code I was using discards any entries that go over the model's max input length. As it does this, it prints out the entry that got discarded. Right off the bat, I noticed:

- A _lot_ of Russian. Did not expect that.
- Excessive repetitions ("!!!!??!!!??????......" and things of the sort)
- `END_OF_DIALOG`, `START:text_remaining:2` and other random tokens of the sorts.

Might be worth cleaning up from the dataset for future runs. As for the fine-tune itself, getting it to converge is an adventure in and of itself. Some chronological notes as I'm doing this:

- Need extremely high smoothing (0.9+) on Tensorboard to be able to spot any patterns in the metrics
- Original hyperparams used for the last few 6B runs don't seem to do well here
- Tried bumping batch size up to 64, then down to 4, then up to 64 again with a different LR...
- Final hyperparams: effective batch size of 64, LR 1e-6.
  - _Lots_ of overflows. However, seems to be converging? I'll leave it for a bit.
  - Actually, for some reason the run keeps crashing around the same step:
    ```
    Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
    [E ProcessGroupNCCL.cpp:461] To avoid data inconsistency, we are taking the entire process down.
    ```
    Gonna attempt to skip over it.
  - ...happened again _way_ later on in the run after I had spread out checkpoint saving to save time, fml
- Run finished. Took much longer than the UFT runs (~4x longer). Loss curve is spiky as hell, but it does trend down if you squint hard enough.

### Results

Sampling settings used:

```bash
# no typical sampling/contrastive search
temperature=0.85, top_p=0.9, repetition_penalty=1.03, top_k=40
```

Testing notes:

- Model seems more attentive to user input, seems it is much harder to get "ignored" with this version vs. the previous ones.
- At the same time, W++ doesn't seem to work as well compared to experiments 1-3. Possible sign of overfitting?
- It's really easy to get the model stuck giving out short responses, unless you take care to write out detailed messages for the greeting and example chat.

Either way, pushed this experiment to the `dev` branch on HuggingFace.

## Experiment 5

### What's new

- A little more CC data. Didn't pay attention to _exactly_ how much extra this time. Around 50MB I think?
- Dropped all non-English data. Around ~2% entries gone, doubt this is even worth the extra preprocessing time.
- Changed SFT loss calculation to look _exclusively_ at the response instead of averaging response + prompt, just to see what that does.
- Experiment 4 overfitted. Having no eval metrics sure is great! Will try to take some inspiration from OpenAI's InstructGPT fine-tune hyperparameters to see how it goes, so:
  - Batch size of 32
  - LR of 9.65e-6, warming up for 500 steps then decaying over the rest of the epoch with a cosine scheduler.

### Training

- Got ~70% of the way through the first attempt then run crashed due to a NCCL timeout.
  - Decided to give the checkpoint a shot but generation quality was _atrocious_.
  - Upon further inspection, I noticed that generations actually started off just fine, but went on for too long and started trailing off.
  - After investigating the tokenization code, I noticed that EOS tokens were not being used - `\n` was being assumed as being the EOS token, which is not the case since our models are multiline. Oops.
- Fixed the above and started another training run, this time with eval metrics thanks to TearGosling.

### Results

Base settings:

```bash
# no typical sampling/contrastive search
temperature=0.6, top_p=0.9, repetition_penalty=1, top_k=0
```

Testing notes:

- Temperature drastically influences how well W++ works.
  - Turning it up enough from the default worsens W++ noticeably
  - Turning it _down_ from the default doesn't seem to worsen it that much, but does result in boring responses
  - ~0.6 might be a sweet spot? Responses are rarely blunt one-word replies, but model still seems to respect the W++ persona well enough
- Repetition penalty influences W++ as well, which seems pretty obvious. Increasing it _at all_ makes W++ worse - and probably normal persona as well depending on how it's worded.
- Putting negatives in the character persona doesn't seem to work that well, but I've noticed this in the previous experiments as well.
  - For example, if adding `Dislikes("Classical piano music")` in the persona, the character will say they don't like classical if you ask them about it like "what do you think about classical music?", but if you instead give them a leading question or ask in a positive manner ("do you like classical music?") it'll respond positively most of the time.
  - This seems to have improved somewhat in this experiment, though. Sampling settings mentioned above result in the character responding correctly most of the time.

TL;DR: More of the same. Pushing to HF for the sake of completeness, but I think any noticeable improvements to the model will likely need to come from the data and prompting instead of more fiddling with the training hyperparameters.
