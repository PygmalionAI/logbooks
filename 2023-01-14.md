# 2023-01-14

Tales of trying out SFT.

## Experiment 4

### What's new

A little more CC data compared to the last run, plus I'm using convogpt's SFT loss implementation instead of a regular CLM loss as the training objective this time.

In more straightforward terms: the model was trained to predict responses to a given input, rather than trying to learn to predict the response _and_ the input prompt all mushed together.

### Training

As an initial note, the SFT code I was using discards any entries that go over the model's max input length. As it does this, it prints out the entry that got discarded. Right off the bat, I noticed:

- A _lot_ of Russian. Did not expect that.
- Excessive repetitions ("!!!!??!!!??????......" and things of the sort)
- `END_OF_DIALOG`, `START:text_remaining:2` and other random tokens of the sorts.

Might be worth cleaning up from the dataset for future runs. As for the fine-tune itself, getting it to converge is an adventure in and of itself. Some chronological notes as I'm doing this:

- Need extremely high smoothing (0.9+) on Tensorboard to be able to spot any patterns in the metrics
- Original hyperparams used for the last few 6B runs don't seem to do well here
- Tried bumping batch size up to 64, then down to 4, then up to 64 again with a different LR...
- Final hyperparams: effective batch size of 64, LR 1e-6.
  - _Lots_ of overflows. However, seems to be converging? I'll leave it for a bit.
  - Actually, for some reason the run keeps crashing around the same step:
    ```
    Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
    [E ProcessGroupNCCL.cpp:461] To avoid data inconsistency, we are taking the entire process down.
    ```
    Gonna attempt to skip over it.
  - ...happened again _way_ later on in the run after I had spread out checkpoint saving to save time, fml
- Run finished. Took much longer than the UFT runs (~4x longer). Loss curve is spiky as hell, but it does trend down if you squint hard enough.

### Results

Sampling settings used:

```bash
# no typical sampling/contrastive search
temperature=0.85, top_p=0.9, repetition_penalty=1.03, top_k=40
```

Testing notes:

- Model seems more attentive to user input, seems it is much harder to get "ignored" with this version vs. the previous ones.
- At the same time, W++ doesn't seem to work as well compared to experiments 1-3. Possible sign of overfitting?
- It's really easy to get the model stuck giving out short responses, unless you take care to write out detailed messages for the greeting and example chat.

Either way, pushed this experiment to the `dev` branch on HuggingFace.
