# 2023-03-03

## Experiment 7

Bit of a hiatus since the last entry because our main training machine was down for over a week.

Either way, feedback for v7 (experiment 6, for those who are understandably confused) was overwhelmingly consistent: the model degenerates to overly-brief responses way too easily. The ~gigabyte of data that the first v7 checkpoint saw was representative of the entire dataset, so I don't think training for around a month on the remaining data makes any sense given the problems we've already spotted.

That being the case, I have opted to abandon v7 for now and instead try again with a new version of the dataset.

### What's new

New data and more strict filtering. Notably:

- **Short responses are aggressively trimmed out of the dataset.**
  - Any training examples where the median response length falls under a certain threshold are dropped completely, and as median response length increases, the chances of dropping the training example diminish. This will hopefully skew the model towards longer responses by default.
- The above dropped the size of our dataset by a few gigabytes, so to compensate we:
  - Added some more instruction following data.
  - Incorporated all freshly submitted community contributed chat logs.

### Training

#### Overview

Nothing new this time, kept everything the same as the previous experiment. So:

- Setup:
  - 4 x 48GB RTX A6000s
  - DeepSpeed with ZeRO stage 1, bf16
- Hyperparameters:
  - Effective batch size of 256
  - Learning rate of 0.98e-5
  - Constant learning rate, with 24 warmup steps

#### Part 1/10

**Training notes:**

So slow! Training loop ran for 3d 8h 58m 42s. Definitely think this could be optimized. Either way, loss curves look good.

**Testing notes:**

Sampling settings:

- **Temperature:** Usually 0.5 to 0.7, bumping up to 0.8-1.0 every now and again to see what happens.
- **Top P sampling:** Either 0.9 or disabled (1.0).
- **Top K samlping:** Either 0 or 40.
- **Repetition penalty:** Between disabled (1.0) and 1.2.
  - Sometimes keeping the repetition penalty disabled works great. Eventually the model will catch on to some mannerism and start repeating it though, so defaulting to something higher than 1.0 seems ideal.

Results:

- Ultra-short responses like "...", "Yes.", "Hm?" seem a lot rarer now!
- If using an assistant-like character and trying to use the model like it's instruction-tuned:
  - Zero-shot doesn't work _that_ well (needs quite a few regens for something good to come up)
  - Few-shot works surprisingly well
  - Anything that requires a long, complete response (e.g. cooking recipe, functional piece of code) is prone to hallucinations
  - Would like to try the above with external knowledge grounding to see whether hallucinations become rarer
- Didn't play with the model for very long. Will release on HF and see what the community has to say.
